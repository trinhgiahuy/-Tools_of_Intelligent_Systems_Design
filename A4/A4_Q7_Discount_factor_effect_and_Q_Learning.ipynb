{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Discount factor effect and Q-Learning"
      ],
      "metadata": {
        "id": "Uj3ZwlkRQKpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Based on the probabilities on the arrows above, Model your MDP (transition probabilities and rewards) in the notebook. you can use the [s, a, sâ€™] for each part or you can use your own way of defining the MDP. What should be seen are transition probabilities, rewards and possible actions"
      ],
      "metadata": {
        "id": "JUL_pNnRBOb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transition_probabilities = {\n",
        "    's0': {'a0': [(0.7, 's0', 10), (0.3, 's1', 0)],\n",
        "           'a1': [(1.0, 's0', 0)],\n",
        "           'a2': [(0.8, 's0', 0), (0.2, 's1', 0)]},\n",
        "\n",
        "    's1': {'a0': [(1.0, 's1', 0)],\n",
        "           'a2': [(1.0, 's2', -50)]},\n",
        "\n",
        "    's2': {'a1': [(0.1, 's2', 0), (0.1,'s1',0), (0.8, 's0', 40)]}\n",
        "}\n",
        "# Add reward to the transition_probabilities\n",
        "# rewards\n",
        "\n",
        "possible_actions = {'s0': ['a0', 'a1', 'a2'],\n",
        "                    's1': ['a0', 'a2'],\n",
        "                    's2': ['a1']}\n",
        "\n",
        "states = ['s0', 's1', 's2']\n",
        "actions = ['a0', 'a1', 'a2']"
      ],
      "metadata": {
        "id": "gQ8nWasjQK-x"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_to_idx = {state: idx for idx, state in enumerate(states)}\n",
        "action_to_idx = {action: idx for idx, action in enumerate(actions)}\n",
        "print(f\"state_to_idx: {state_to_idx}\")\n",
        "print(f\"action_to_idx: {action_to_idx}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dlr97fF12KdB",
        "outputId": "7929b3be-0bce-4e4a-965c-a045f2fdf0d1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "state_to_idx: {'s0': 0, 's1': 1, 's2': 2}\n",
            "action_to_idx: {'a0': 0, 'a1': 1, 'a2': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for state in transition_probabilities:\n",
        "    for action in transition_probabilities[state]:\n",
        "        prob_sum = sum([prob for prob, _, _ in transition_probabilities[state][action]])\n",
        "        if not np.isclose(prob_sum, 1.0):\n",
        "            print(f\"Warning: Probabilities for state {state} action {action} do not sum to 1: {prob_sum}\")\n"
      ],
      "metadata": {
        "id": "JkFqkJCuQPOi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q_values = np.full((3, 3), -np.inf)  # -np.inf for impossible actions\n",
        "# for state, actions in enumerate(possible_actions):\n",
        "#     Q_values[state, actions] = 0.0  # for all possible actions\n",
        "import numpy as np\n",
        "Q_values = np.full((len(states), len(actions)), -np.inf)  # 3 states, 3 actions\n",
        "for state in possible_actions:\n",
        "    for action in possible_actions[state]:\n",
        "        Q_values[state_to_idx[state], action_to_idx[action]] = 0.0"
      ],
      "metadata": {
        "id": "-mcUQqriQLoW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Transition Probabilities are valid.\")\n",
        "print(f\"state_to_idx: {state_to_idx}\")\n",
        "print(f\"action_to_idx: {action_to_idx}\")\n",
        "print(\"Initialized Q_values:\")\n",
        "print(Q_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hshuBIup9BII",
        "outputId": "492c3ad0-afd6-4a06-b842-4962a04f5fd4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transition Probabilities are valid.\n",
            "state_to_idx: {'s0': 0, 's1': 1, 's2': 2}\n",
            "action_to_idx: {'a0': 0, 'a1': 1, 'a2': 2}\n",
            "Initialized Q_values:\n",
            "[[  0.   0.   0.]\n",
            " [  0. -inf   0.]\n",
            " [-inf   0. -inf]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ANSWER Q7.1**:\n",
        "* In transition probabilities, I defined `transition_probabilities` is a dictionaries of keys are initial states (s0, s1, s2) and values are dictionaries including keys are actions (a0, a1, a2) which are depended on the states (each states have specific actions to take), and values are list in form of `[probabilities, next_state, reward]`\n",
        "\n",
        "* The `possible_actions` include specific actions corresponding to each states that can be taken\n"
      ],
      "metadata": {
        "id": "wOboYDX3IZEJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Take your discount factor to be 0.9. Perform Q-learning and report the Q-values for each (state, action) pair. Based on that, what is the optimal policy?"
      ],
      "metadata": {
        "id": "y1aW8WF16GOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.9  # the discount factor\n",
        "\n",
        "#gamma = 0.95\n",
        "for iteration in range(50):\n",
        "    # --- fill here (perform a DP approach for filling up your Q-table (repeat the process by stting gamma to be 0.95 )\n",
        "    new_Q_values = np.copy(Q_values)\n",
        "    for state in states:\n",
        "      state_idx = state_to_idx[state]\n",
        "      for action in possible_actions[state]:\n",
        "        action_idx = action_to_idx[action]\n",
        "        value_sum = 0\n",
        "        for prob, next_state, reward in transition_probabilities[state][action]:\n",
        "          next_state_idx = state_to_idx[next_state]\n",
        "          best_next_action = np.argmax(new_Q_values[next_state_idx])\n",
        "          value_sum += prob * (reward + gamma * new_Q_values[next_state_idx][best_next_action])\n",
        "\n",
        "        new_Q_values[state_idx, action_idx] = value_sum\n",
        "    Q_values = new_Q_values"
      ],
      "metadata": {
        "id": "zx3UhYrBQNiu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Q-values with gamma = 0.9:\")\n",
        "print(Q_values)"
      ],
      "metadata": {
        "id": "gdRbvTjMQTWZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "565c163f-a350-473e-98fb-40fcb1de1a8f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-values with gamma = 0.9:\n",
            "[[18.91891892 17.02702703 13.62162162]\n",
            " [ 0.                -inf -4.87971488]\n",
            " [       -inf 50.13365013        -inf]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q_values.argmax(axis=1)  # optimal action for each state\n",
        "optimal_policy = [actions[np.argmax(Q_values[state_to_idx[state]])] for state in states]\n",
        "print(\"Optimal policy with gamma = 0.9:\")\n",
        "print(optimal_policy)"
      ],
      "metadata": {
        "id": "lGHSyhS3QV_4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e270b31-8281-4132-f4e9-42ff7eaecb8c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal policy with gamma = 0.9:\n",
            "['a0', 'a0', 'a1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ANSWER Q7.2**:\n",
        "The highest Q-value for `s0` is for action `a0`, for `s1` is action `a0` and for `s2` is action `a1` with Q value of 50.13.\n",
        "\n",
        "Thus the optimal policy is\n",
        "\n",
        "* From state s0, the optimal action is a0.\n",
        "* From state s1, the optimal action is a0.\n",
        "* From state s2, the optimal action is a1.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EnFxzUIEJa0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Perform the same procedure but this time with a discount factor of 0.95. Did your optimal policy change? Explain your results.\n",
        "\n"
      ],
      "metadata": {
        "id": "cyS6xw6MCu8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# you can use the same code as above\n",
        "Q_values = np.full((len(states), len(actions)), -np.inf)  # Reset Q-values\n",
        "for state in possible_actions:\n",
        "    for action in possible_actions[state]:\n",
        "        Q_values[state_to_idx[state], action_to_idx[action]] = 0.0\n",
        "\n",
        "\n",
        "gamma = 0.95  # New discount factor\n",
        "for iteration in range(50):\n",
        "    # --- fill here (perform a DP approach for filling up your Q-table (repeat the process by stting gamma to be 0.95 )\n",
        "    new_Q_values = np.copy(Q_values)\n",
        "    for state in states:\n",
        "      state_idx = state_to_idx[state]\n",
        "      for action in possible_actions[state]:\n",
        "        action_idx = action_to_idx[action]\n",
        "        value_sum = 0\n",
        "        for prob, next_state, reward in transition_probabilities[state][action]:\n",
        "          next_state_idx = state_to_idx[next_state]\n",
        "          best_next_action = np.argmax(new_Q_values[next_state_idx])\n",
        "          value_sum += prob * (reward + gamma * new_Q_values[next_state_idx][best_next_action])\n",
        "\n",
        "        new_Q_values[state_idx, action_idx] = value_sum\n",
        "    Q_values = new_Q_values"
      ],
      "metadata": {
        "id": "0xwcXCskC0aD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print Q-values\n",
        "print(\"Q-values with gamma = 0.95:\")\n",
        "print(Q_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6iH22Vw-pML",
        "outputId": "b6f1e591-26eb-4562-fa9e-dd42f44c2d5a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-values with gamma = 0.95:\n",
            "[[21.79615996 20.70635196 16.76923123]\n",
            " [ 1.02074831        -inf  1.08097586]\n",
            " [       -inf 53.77587186        -inf]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_policy = [actions[np.argmax(Q_values[state_to_idx[state]])] for state in states]\n",
        "print(\"Optimal policy with gamma = 0.95:\")\n",
        "print(optimal_policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76uIWW1F-qhQ",
        "outputId": "bd188348-3e5e-464e-ddc9-dc6a35f916a4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal policy with gamma = 0.95:\n",
            "['a0', 'a2', 'a1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ANSWER Q7.3**:\n",
        "\n",
        "The highest Q-value for `s0` is for action `a0`, for `s1` is action `a2` and for `s2`, action `a1` has a Q-value of 53.78.\n",
        "\n",
        "Now the optimal policy is:\n",
        "* From state s0, the optimal action is a0.\n",
        "* From state s1, the optimal action is a2.\n",
        "* From state s2, the optimal action is a1.\n",
        "\n"
      ],
      "metadata": {
        "id": "JrcAQ3ErKfmi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Detail analysis and result explanation**\n",
        "\n",
        "For discount factor `gamma = 0.95`, the optimal policy changes, especially in state 2 from taking action `a0` (`gamma = 0.9`) to action `a2`. The changes is due to higher discount factors enphasize more importance on future reward. This indicates a consideration of future rewards despite the immediate cost of -50. This is likely due to reaching s2 has a potential high future reward (the transition from s2 back to s0 with a1).\n"
      ],
      "metadata": {
        "id": "x0lulfyXK8im"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ANSWER Q7.4**:\n",
        "\n",
        "Discount factor determined the importance of future rewards in the calculation of total expected reward. It is used in Q-learning update formula:\n",
        "\n",
        "$$\n",
        "Q(s, a) = \\sum_{s'} P(s' \\mid s, a) \\left[ R(s, a, s') + \\gamma \\max_{a'} Q(s', a') \\right]\n",
        "$$\n",
        "\n",
        "The discount factor $\\gamma$ is a value between 0 and 1:\n",
        "- When $\\gamma$ is close to 1, future rewards are highly valued, encouraging long-term strategy.\n",
        "- When $\\gamma$ is close to 0, immediate rewards are prioritized, focusing on short-term gains.\n",
        "\n",
        "if we modify the discount factor, the optimal policy can changed based on the value of $\\gamma$ as we demonstrated above\n",
        "\n"
      ],
      "metadata": {
        "id": "zGY-IlHIMBOK"
      }
    }
  ]
}